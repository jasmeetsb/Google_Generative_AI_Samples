{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKcLm7WC7o9s"
   },
   "source": [
    "# Notebook showcasing use of Google Cloud GenAI(Palm API) + Doc AI to extract information from documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Jasmeet Bhatia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective:** In this notebook we will use Veretx AI PALM Text model to extract intentities from a scanned PDF containing Patent Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70lij66aJZoV",
    "outputId": "4341d94e-5fc2-48a9-9c1a-6ed29ae62b16"
   },
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "!pip install google-cloud-aiplatform --upgrade\n",
    "#!apt-get install poppler-utils\n",
    "#!pip install google-cloud-core\n",
    "!pip install google-cloud-documentai\n",
    "#!pip install google-cloud-storage\n",
    "#!pip install simplejson\n",
    "#!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GeB5akYjIOnG",
    "outputId": "086e434c-35a9-478c-ca6d-394916ca91b9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tvj408TIDbd"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "#from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import pandas as pd\n",
    "\n",
    "#from PIL import Image, ImageDraw\n",
    "#import os\n",
    "\n",
    "#from IPython.display import display, Image\n",
    "#from pdf2image import convert_from_path, convert_from_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWi0T7F7JQmj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use if running notebook locally\n",
    "#! gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03otjfCP8Eh2"
   },
   "outputs": [],
   "source": [
    "##Run only if using Google Colab Notebooks\n",
    "#from google.colab import auth as google_auth\n",
    "#google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define path to the pdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and review the PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='./sample_data/34_Deed.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(file_path, width=800, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "x9GfXM3XGy86",
    "outputId": "a87f25ea-d638-42e2-db0e-e4fc6f1a3635"
   },
   "source": [
    "### Use GCP Document AI to OCR the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVni3cEcXPkV"
   },
   "outputs": [],
   "source": [
    "# Define function to OCR the PDF using Document AI\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    field_mask: str = None,\n",
    "):\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Import the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load the image content\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "\n",
    "    document = result.document\n",
    "\n",
    "    # Read the text recognition output from the processor\n",
    "    return(document.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LpZ91pqHMco"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEV5JaTZXPne"
   },
   "outputs": [],
   "source": [
    "#For PDF Docs\n",
    "ocr_output = process_document_sample(\n",
    "  project_id=\"398507275014\",\n",
    "  location=\"us\",\n",
    "  processor_id=\"2fb6b1be15c7f2d\",\n",
    "    mime_type = 'application/pdf',\n",
    "    field_mask = None,\n",
    "  file_path= file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2rk-TcvWIx"
   },
   "outputs": [],
   "source": [
    "##For TIFF docs uncomment the below section and run\n",
    "#ocr_output = process_document_sample(\n",
    "#  project_id=\"398507275014\",\n",
    "#  location=\"us\",\n",
    "#  processor_id=\"2fb6b1be15c7f2d\",\n",
    "#    mime_type = 'image/tiff',\n",
    "#    field_mask = None,\n",
    "#  file_path=\"./genai_demo_data/demo_data.tiff\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4N90n_fLXUQK",
    "outputId": "96e32946-0928-4210-8b62-a58943f1e7be"
   },
   "outputs": [],
   "source": [
    "#Print the first 1000 characters of the OCR output\n",
    "print(ocr_output[:32000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylSAjPgvHp4k"
   },
   "source": [
    "### Run the OCR results above through the Vertex AI GenAI/PALM Model to extact entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlK_Egd47rKn"
   },
   "outputs": [],
   "source": [
    "# Define the function to process OCR output through Vertex AI GenAI Model\n",
    "\n",
    "\n",
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    print(f\"Response from Model: {response.text}\")\n",
    "    return(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the prompt to be used for entity etxraction from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_suffix = '''Give me following information extracted form text above in a Table format:\n",
    "- Name of Seller 1\n",
    "- Seller 1 Type\n",
    "-  If seller is an LLC, then name of the officer of the LLC\n",
    "- Name of Seller 2\n",
    "- Name of buyer 1\n",
    "- Name of buyer 2\n",
    "- Name of buyer 3\n",
    "- Type of ownership\n",
    "- Name of Title Company\n",
    "- Address of the property\n",
    "- Tract Number\n",
    "- Water Right Details\n",
    "- Title Order number\n",
    "- Document transfer tax'''\n",
    "\n",
    "#- List of parcels\n",
    "#- Property details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the OCR output and the prompt/question above to create full input text to be fet to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "rR3djNdVZsTs",
    "outputId": "412e3b47-57b0-4a89-e04f-7e06797843ff"
   },
   "outputs": [],
   "source": [
    "ocr_text = ocr_output+prompt_suffix\n",
    "print(ocr_text[5000:20000]) #Limiting to 20K characters in teh notebook. Model can handle 8K Tokens = ~32K Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the input prompt to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMmq3tfaW5c",
    "outputId": "9fc304a3-8066-4d34-fc66-13fa6bccbdac"
   },
   "outputs": [],
   "source": [
    "# Process the full Input Text through the GenAI Model\n",
    "llm_output1 = predict_large_language_model_sample(\"jsb-alto\", #GCP Project\n",
    "                                                 \"text-bison@001\", #LLM Model \n",
    "                                                 0.2, #Temperature\n",
    "                                                 256, #Max output tokens\n",
    "                                                 0.8, #Top K\n",
    "                                                 40,  #Top P\n",
    "                                                 ocr_text, \n",
    "                                                 \"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_suffix = ''' Convert the above information into table format with \n",
    "Columns - (Seller_1, Seller_1_Type, Seller_1_Officer, Seller_2, Buyer_1, Buyer_2, Buyer_3, Type_of_Ownership,Title_Company,Title_order_number,Document_transfer_tax)\n",
    "For Blank fields put N/A'''\n",
    "\n",
    "prompt2 = llm_output1+prompt_suffix\n",
    "print(prompt2[:20000]) #Limiting to 20K characters in teh notebook. Model can handle 8K Tokens = ~32K Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output2 = predict_large_language_model_sample(\"jsb-alto\", #GCP Project\n",
    "                                                 \"text-bison@001\", #LLM Model \n",
    "                                                 0.2, #Temperature\n",
    "                                                 256, #Max output tokens\n",
    "                                                 0.8, #Top K\n",
    "                                                 40,  #Top P\n",
    "                                                 prompt2, \n",
    "                                                 \"us-central1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-fT8JlIag9E",
    "outputId": "ce8aaa70-6470-4835-f9ab-7f6fa321e78b"
   },
   "outputs": [],
   "source": [
    "#Print the answer received from LLM. \n",
    "#In this Patent document use case, answer should the name of the inventors\n",
    "print(llm_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert to PD Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkB9bacP2b_Q"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "output = pd.read_csv(io.StringIO(llm_output2), sep='|')\n",
    "output = output.dropna(axis=1, how='all')\n",
    "#output = output.dropna(axis=0, how='all')\n",
    "# remove special character\n",
    "output.columns = output.columns.str.replace(' ', '')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Table to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "import pytz\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"jsb-alto.entity_extract.deed_extract6\"\n",
    "\n",
    "\n",
    "dataframe = output\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        # Specify the type of columns whose type cannot be auto-detected. For\n",
    "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
    "        # data type is ambiguous.\n",
    "        bigquery.SchemaField(\"Seller_1\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        # Indexes are written if included in the schema by name.\n",
    "        bigquery.SchemaField(\"Seller_1_Type\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        # Indexes are written if included in the schema by name.\n",
    "        bigquery.SchemaField(\"Seller_1_Officer\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Seller_2\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Buyer_1\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Buyer_2\", bigquery.enums.SqlTypeNames.STRING),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM jsb-alto.entity_extract.deed_extract6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
