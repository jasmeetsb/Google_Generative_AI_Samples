{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKcLm7WC7o9s"
   },
   "source": [
    "# Entity Extraction from Documents using Google Cloud Vertex AI and Document AI\n",
    "\n",
    "This notebook demonstrates how to extract structured information from scanned documents using Google Cloud's Document AI (for OCR) and Vertex AI's PaLM Text model (for entity extraction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Jasmeet Bhatia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook, we will use Google Cloud Vertex AI's PaLM Text model to extract entities from a scanned PDF containing deed information. The process will involve:\n",
    "\n",
    "1. Using Document AI to convert the scanned PDF to text (OCR)\n",
    "2. Applying Vertex AI's language models to extract structured information\n",
    "3. Converting the extracted information to a tabular format\n",
    "4. Storing the results in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70lij66aJZoV",
    "outputId": "4341d94e-5fc2-48a9-9c1a-6ed29ae62b16"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install google-cloud-aiplatform --upgrade\n",
    "!pip install google-cloud-documentai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GeB5akYjIOnG",
    "outputId": "086e434c-35a9-478c-ca6d-394916ca91b9"
   },
   "outputs": [],
   "source": [
    "# Install all dependencies from requirements.txt\n",
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tvj408TIDbd"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from google.cloud import documentai\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import pandas as pd\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define path to the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the sample deed document\n",
    "file_path = './sample_data/34_Deed.pdf'\n",
    "\n",
    "## Display and review the PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='./sample_data/34_Deed.pdf'\n",
    "\n",
    "# Display the PDF document for review\n",
    "from IPython.display import IFrame\n",
    "IFrame(file_path, width=800, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document AI OCR Processing\n",
    "\n",
    "Use Google Cloud Document AI to perform OCR on the PDF document and extract text.\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(file_path, width=800, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use GCP Document AI to OCR the PDF\n",
    "\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    field_mask: str = None,\n",
    "):\n",
    "    \"\"\"Process a document using Document AI.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Google Cloud Project ID\n",
    "        location: Location of the Document AI processor\n",
    "        processor_id: ID of the Document AI processor\n",
    "        file_path: Path to the document to process\n",
    "        mime_type: MIME type of the document (e.g., 'application/pdf')\n",
    "        field_mask: Optional field mask\n",
    "    \n",
    "    Returns:\n",
    "        The extracted text from the document\n",
    "    \"\"\"\n",
    "    # Create Document AI client\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "    \n",
    "    # Get the full resource name of the processor\n",
    "    name = client.processor_path(project_id, location, processor_id)\n",
    "    \n",
    "    # Import the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "    \n",
    "    # Load the image content\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "    \n",
    "    # Process the document\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name, raw_document=raw_document\n",
    "    )\n",
    "    \n",
    "    result = client.process_document(request=request)\n",
    "    document = result.document\n",
    "    \n",
    "    # Return the extracted text\n",
    "    return document.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVni3cEcXPkV"
   },
   "outputs": [],
   "source": [
    "### Process PDF Document\n",
    "\n",
    "# Define function to OCR the PDF using Document AI\n",
    "def process_document_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    "    field_mask: str = None,\n",
    "):\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient()\n",
    "\n",
    "    name = client.processor_path(project_id, location, processor_id)\n",
    "\n",
    "    # Import the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load the image content\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name, raw_document=raw_document\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "\n",
    "    document = result.document\n",
    "\n",
    "    # Read the text recognition output from the processor\n",
    "    return(document.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for PDF Files\n",
    "\n",
    "### Alternative: Process TIFF Document\n",
    "\n",
    "If working with TIFF files instead of PDFs, use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEV5JaTZXPne"
   },
   "outputs": [],
   "source": [
    "#For PDF Docs\n",
    "ocr_output = process_document_sample(\n",
    "  project_id=\"398507275014\",\n",
    "  location=\"us\",\n",
    "  processor_id=\"2fb6b1be15c7f2d\",\n",
    "    mime_type = 'application/pdf',\n",
    "    field_mask = None,\n",
    "  file_path= file_path\n",
    ")\n",
    "\n",
    "# Uncomment and modify to process TIFF documents\n",
    "# ocr_output = process_document_sample(\n",
    "#     project_id=\"398507275014\",\n",
    "#     location=\"us\",\n",
    "#     processor_id=\"2fb6b1be15c7f2d\",\n",
    "#     mime_type='image/tiff',\n",
    "#     field_mask=None,\n",
    "#     file_path=\"./sample_data/sample_deed.tiff\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for TIFF files\n",
    "\n",
    "# Preview the OCR output (first 32,000 characters)\n",
    "print(ocr_output[:32000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-2rk-TcvWIx"
   },
   "outputs": [],
   "source": [
    "## Entity Extraction with Vertex AI\n",
    "\n",
    "Now we'll use Vertex AI's PaLM model to extract structured information from the OCR text.\n",
    "\n",
    "##For TIFF docs uncomment the below section and run\n",
    "#ocr_output = process_document_sample(\n",
    "#  project_id=\"398507275014\",\n",
    "#  location=\"us\",\n",
    "#  processor_id=\"2fb6b1be15c7f2d\",\n",
    "#    mime_type = 'image/tiff',\n",
    "#    field_mask = None,\n",
    "#  file_path=\"./genai_demo_data/demo_data.tiff\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4N90n_fLXUQK",
    "outputId": "96e32946-0928-4210-8b62-a58943f1e7be"
   },
   "outputs": [],
   "source": [
    "#Print the first 1000 characters of the OCR output\n",
    "print(ocr_output[:32000])\n",
    "\n",
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    ") :\n",
    "    \"\"\"Generate predictions from a Large Language Model.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Google Cloud Project ID\n",
    "        model_name: Name of the language model to use\n",
    "        temperature: Sampling temperature (higher = more creative, lower = more deterministic)\n",
    "        max_decode_steps: Maximum number of tokens to generate\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-k sampling parameter\n",
    "        content: The input content/prompt to send to the model\n",
    "        location: Google Cloud region\n",
    "        tuned_model_name: Optional tuned model name if using a fine-tuned model\n",
    "        \n",
    "    Returns:\n",
    "        The model's text response\n",
    "    \"\"\"\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    \n",
    "    # Load the model\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    \n",
    "    # Generate prediction\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    \n",
    "    print(f\"Response from Model: {response.text}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylSAjPgvHp4k"
   },
   "source": [
    "### Create Extraction Prompt\n",
    "\n",
    "Define the prompt that will guide the LLM to extract specific entities from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlK_Egd47rKn"
   },
   "outputs": [],
   "source": [
    "# Define the function to process OCR output through Vertex AI GenAI Model\n",
    "\n",
    "\n",
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "      model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    print(f\"Response from Model: {response.text}\")\n",
    "    return(response.text)\n",
    "\n",
    "# Define the prompt for entity extraction\n",
    "prompt_suffix = '''Give me following information extracted from text above in a Table format:\n",
    "- Name of Seller 1\n",
    "- Seller 1 Type\n",
    "- If seller is an LLC, then name of the officer of the LLC\n",
    "- Name of Seller 2\n",
    "- Name of buyer 1\n",
    "- Name of buyer 2\n",
    "- Name of buyer 3\n",
    "- Type of ownership\n",
    "- Name of Title Company\n",
    "- Address of the property\n",
    "- Tract Number\n",
    "- Water Right Details\n",
    "- Title Order number\n",
    "- Document transfer tax'''\n",
    "\n",
    "# Additional fields that could be extracted if needed:\n",
    "# - List of parcels\n",
    "# - Property details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine OCR Text with Prompt\n",
    "\n",
    "Merge the extracted text with our prompt to create the input for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_suffix = '''Give me following information extracted form text above in a Table format:\n",
    "- Name of Seller 1\n",
    "- Seller 1 Type\n",
    "-  If seller is an LLC, then name of the officer of the LLC\n",
    "- Name of Seller 2\n",
    "- Name of buyer 1\n",
    "- Name of buyer 2\n",
    "- Name of buyer 3\n",
    "- Type of ownership\n",
    "- Name of Title Company\n",
    "- Address of the property\n",
    "- Tract Number\n",
    "- Water Right Details\n",
    "- Title Order number\n",
    "- Document transfer tax'''\n",
    "\n",
    "# Combine OCR output with our prompt\n",
    "ocr_text = ocr_output + prompt_suffix\n",
    "\n",
    "# Preview a portion of the input text\n",
    "# Note: Limiting to 15K characters in the notebook. Model can handle 8K tokens (~32K characters)\n",
    "print(ocr_text[5000:20000])\n",
    "\n",
    "#- List of parcels\n",
    "#- Property details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute First LLM Query\n",
    "\n",
    "Send the combined OCR text and prompt to the language model for initial entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "rR3djNdVZsTs",
    "outputId": "412e3b47-57b0-4a89-e04f-7e06797843ff"
   },
   "outputs": [],
   "source": [
    "ocr_text = ocr_output+prompt_suffix\n",
    "print(ocr_text[5000:20000]) #Limiting to 20K characters in teh notebook. Model can handle 8K Tokens = ~32K Characters\n",
    "\n",
    "# Process the OCR text through the Vertex AI model\n",
    "llm_output1 = predict_large_language_model_sample(\n",
    "    project_id=\"jsb-alto\",         # GCP Project\n",
    "    model_name=\"text-bison@001\",   # LLM Model \n",
    "    temperature=0.2,                # Temperature (lower = more deterministic)\n",
    "    max_decode_steps=256,           # Max output tokens\n",
    "    top_p=0.8,                      # Top-p sampling parameter\n",
    "    top_k=40,                       # Top-k sampling parameter\n",
    "    content=ocr_text,               # Input content\n",
    "    location=\"us-central1\"         # GCP region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the input prompt to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYMmq3tfaW5c",
    "outputId": "9fc304a3-8066-4d34-fc66-13fa6bccbdac"
   },
   "outputs": [],
   "source": [
    "# Process the full Input Text through the GenAI Model\n",
    "llm_output1 = predict_large_language_model_sample(\"jsb-alto\", #GCP Project\n",
    "                                                 \"text-bison@001\", #LLM Model \n",
    "                                                 0.2, #Temperature\n",
    "                                                 256, #Max output tokens\n",
    "                                                 0.8, #Top K\n",
    "                                                 40,  #Top P\n",
    "                                                 ocr_text, \n",
    "                                                 \"us-central1\")\n",
    "\n",
    "# Define the second prompt for table formatting\n",
    "prompt_suffix = '''Convert the above information into table format with \n",
    "Columns - (Seller_1, Seller_1_Type, Seller_1_Officer, Seller_2, Buyer_1, Buyer_2, Buyer_3, Type_of_Ownership, Title_Company, Title_order_number, Document_transfer_tax)\n",
    "For blank fields put N/A'''\n",
    "\n",
    "# Combine the first response with the formatting prompt\n",
    "prompt2 = llm_output1 + prompt_suffix\n",
    "\n",
    "# Preview a portion of the combined text\n",
    "print(prompt2[:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Results as a Table\n",
    "\n",
    "Now we'll ask the model to format the extracted information into a structured table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_suffix = ''' Convert the above information into table format with \n",
    "Columns - (Seller_1, Seller_1_Type, Seller_1_Officer, Seller_2, Buyer_1, Buyer_2, Buyer_3, Type_of_Ownership,Title_Company,Title_order_number,Document_transfer_tax)\n",
    "For Blank fields put N/A'''\n",
    "\n",
    "prompt2 = llm_output1+prompt_suffix\n",
    "print(prompt2[:20000]) #Limiting to 20K characters in teh notebook. Model can handle 8K Tokens = ~32K Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Second LLM Query\n",
    "\n",
    "Send the second prompt to the language model to format the extracted data as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output2 = predict_large_language_model_sample(\"jsb-alto\", #GCP Project\n",
    "                                                 \"text-bison@001\", #LLM Model \n",
    "                                                 0.2, #Temperature\n",
    "                                                 256, #Max output tokens\n",
    "                                                 0.8, #Top K\n",
    "                                                 40,  #Top P\n",
    "                                                 prompt2, \n",
    "                                                 \"us-central1\")\n",
    "\n",
    "### View Tabular Results\n",
    "\n",
    "Display the formatted table response from the language model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the formatted table received from the LLM\n",
    "print(llm_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-fT8JlIag9E",
    "outputId": "ce8aaa70-6470-4835-f9ab-7f6fa321e78b"
   },
   "outputs": [],
   "source": [
    "# Print the answer received from LLM. \n",
    "# In this Patent document use case, answer should the name of the inventors\n",
    "print(llm_output2)\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "### Convert to Pandas DataFrame\n",
    "\n",
    "Transform the table-formatted text into a structured DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to PD Dataframe\n",
    "\n",
    "# Parse the pipe-delimited output into a pandas DataFrame\n",
    "import io\n",
    "\n",
    "# Convert the string output to a DataFrame\n",
    "output = pd.read_csv(io.StringIO(llm_output2), sep='|')\n",
    "\n",
    "# Clean up the DataFrame\n",
    "output = output.dropna(axis=1, how='all')   # Remove empty columns\n",
    "output.columns = output.columns.str.replace(' ', '')  # Remove spaces from column names\n",
    "\n",
    "# Display the DataFrame\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkB9bacP2b_Q"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "output = pd.read_csv(io.StringIO(llm_output2), sep='|')\n",
    "output = output.dropna(axis=1, how='all')\n",
    "#output = output.dropna(axis=0, how='all')\n",
    "# remove special character\n",
    "output.columns = output.columns.str.replace(' ', '')\n",
    "output\n",
    "\n",
    "## Data Storage\n",
    "\n",
    "### Export Results to BigQuery\n",
    "\n",
    "Store the extracted and structured data in Google BigQuery for further analysis and integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Table to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "import pytz\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "table_id = \"jsb-alto.entity_extract.deed_extract6\"\n",
    "\n",
    "\n",
    "dataframe = output\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        # Specify the type of columns whose type cannot be auto-detected. For\n",
    "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
    "        # data type is ambiguous.\n",
    "        bigquery.SchemaField(\"Seller_1\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        # Indexes are written if included in the schema by name.\n",
    "        bigquery.SchemaField(\"Seller_1_Type\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        # Indexes are written if included in the schema by name.\n",
    "        bigquery.SchemaField(\"Seller_1_Officer\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Seller_2\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Buyer_1\", bigquery.enums.SqlTypeNames.STRING),\n",
    "        \n",
    "        bigquery.SchemaField(\"Buyer_2\", bigquery.enums.SqlTypeNames.STRING),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(\n",
    "    dataframe, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), table_id\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run a query to retrieve all stored records\n",
    "query = f\"SELECT * FROM `{table_id}`\"\n",
    "query_job = client.query(query)  # Make an API request.\n",
    "results = query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df = results.to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Saved Data\n",
    "\n",
    "Verify that our data is accessible in BigQuery by running a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT * FROM jsb-alto.entity_extract.deed_extract6"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
