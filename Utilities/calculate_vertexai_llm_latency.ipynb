{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "269f12e3-9815-4d0d-8f5f-75d6faca9d12",
   "metadata": {},
   "source": [
    "## Calculate end-end Latency of different Vertex AI Foundational Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "807fd8c3-0993-485d-b20f-0ad4dc5dfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import supporting libraries\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import time\n",
    "import timeit\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1dc47-22dd-4c7f-bb09-7682d2cf6a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d9bc10c-d1b5-416b-9cd9-0b8ad75c98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set GCP Project and model parameters\n",
    "vertexai.init(project=\"jsb-alto\", location=\"us-central1\")\n",
    "parameters = {\n",
    "    \"candidate_count\": 1,\n",
    "    \"max_output_tokens\": 50,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.5,\n",
    "    \"top_k\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50687444-fc78-4b29-a5e9-8ab89bebea45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a29f951-6d73-4e8b-9eeb-5438385bb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query function to send the text input to a model in Vertex AI and return the response text\n",
    "def query_llm(model_name,input):\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    response = model.predict(\n",
    "    input,\n",
    "    **parameters)\n",
    "    #print(response.text)\n",
    "    return(response.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0129f3df-ed9d-493e-9503-696228bb7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to accept the text input and calculate average response time \n",
    "def test_model_latency(model_name, sample_input,iterations=10):\n",
    "    #model = TextGenerationModel.from_pretrained(model_name)\n",
    "\n",
    "    # Time the execution of the query_llm function\n",
    "    timing = timeit.timeit(lambda: query_llm(model_name,sample_input),\n",
    "                       setup=\"from __main__ import query_llm\",\n",
    "                       number=iterations)\n",
    "\n",
    "    # Print the average execution time\n",
    "    print(f\"Average execution time for {model_name}: {timing / iterations:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deafd8-7434-4093-aaaf-d76c4de60216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88772fd5-d2a8-409e-a75b-52adc6f631ed",
   "metadata": {},
   "source": [
    "### Run a latency test to calculate average response time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ac1f96f-5acb-46ae-9f7d-9b1b12364532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average execution time for text-bison@002: 0.358944 seconds\n"
     ]
    }
   ],
   "source": [
    "test_model_latency(\"text-bison@002\",input,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a30efd-3997-46a0-8586-9e3e759c44f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e7f76be-b206-4329-9ccf-95e21aef3acf",
   "metadata": {},
   "source": [
    "## Calculate response percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad7d0c64-696f-4cd5-8e2d-89e401e5111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to accept the text input and calculate response time percentiles\n",
    "def test_model_latency(model_name, sample_input,iterations=10):\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "\n",
    "    # Create the Timer object\n",
    "    #timer = timeit.Timer(f\"query_llm('{sample_input}')\",setup=\"from __main__ import query_llm\",)\n",
    "    \n",
    "    # Create the Timer object\n",
    "    timer = timeit.Timer(f\"query_llm('{model_name}','{sample_input}')\",setup=\"from __main__ import query_llm\",)\n",
    "\n",
    "    # Run the timer multiple times\n",
    "    times = timer.repeat(repeat=iterations, number=1)\n",
    "\n",
    "    # Calculate percentile or other statistics\n",
    "    import numpy as np\n",
    "    p50 = np.percentile(times, 50)\n",
    "    p90 = np.percentile(times, 90)\n",
    "    p99 = np.percentile(times, 99)\n",
    "    \n",
    "    \n",
    "    print(\"Model: \",model_name)\n",
    "    print(\"Temperature: \",parameters)\n",
    "    print(\"p50:\", p50)\n",
    "    print(\"p90:\", p90)\n",
    "    print(\"p99:\", p99)\n",
    "\n",
    "    # Print the average execution time\n",
    "    #print(f\"Average execution time for {model_name}: {timing / iterations:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4346523a-87e0-43da-91ad-9729f96e8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide input for the model. Replace with your actual input\n",
    "input = \"What is the capital of USA?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b1389af-e0df-4eb6-a568-9f05c5b5dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  text-bison@002\n",
      "Temperature:  {'candidate_count': 1, 'max_output_tokens': 50, 'temperature': 0.2, 'top_p': 0.5, 'top_k': 40}\n",
      "p50: 0.4101508385501802\n",
      "p90: 0.4769554691389203\n",
      "p99: 0.525991853736341\n"
     ]
    }
   ],
   "source": [
    "test_model_latency(\"text-bison@002\",input,iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f5a36-330c-44b6-baf3-32933ec40bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ee053-91df-4827-bc9d-02214be96cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
